{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af1344d-bc76-46af-8444-91812f8a5975",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering, AdamW\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda6e0f1-9868-44ab-b7c4-968d04108166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_squad_data(file_path):\n",
    "    \"\"\"\n",
    "    Load SQuAD data from JSON format and extract contexts, questions, and answers.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    contexts, questions, answers = [], [], []\n",
    "    for article in data['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa_pair in paragraph['qas']:\n",
    "                question = qa_pair['question']\n",
    "                answer_key = 'plausible_answers' if 'plausible_answers' in qa_pair else 'answers'\n",
    "                for answer in qa_pair[answer_key]:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "                    \n",
    "    return contexts, questions, answers\n",
    "\n",
    "# Load train and validation data\n",
    "train_contexts, train_questions, train_answers = load_squad_data('squad/train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers = load_squad_data('squad/test-v2.0.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5521fbf6-5883-490b-bda9-7a7061697881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adjust_answer_end_indices(answers, contexts):\n",
    "    \"\"\"\n",
    "    Adjust answer end indices to match the exact positions in the context.\n",
    "    \"\"\"\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        answer_text = answer['text']\n",
    "        start_index = answer['answer_start']\n",
    "        end_index = start_index + len(answer_text)\n",
    "\n",
    "        # Check if the substring matches the answer text\n",
    "        if context[start_index:end_index] == answer_text:\n",
    "            answer['answer_end'] = end_index\n",
    "        else:\n",
    "            # Try shifting the start index if there's a mismatch\n",
    "            for offset in [1, 2]:\n",
    "                if context[start_index - offset:end_index - offset] == answer_text:\n",
    "                    answer['answer_start'] = start_index - offset\n",
    "                    answer['answer_end'] = end_index - offset\n",
    "                    break\n",
    "            else:\n",
    "                # Default the end index to the end of the answer if no match is found\n",
    "                answer['answer_end'] = start_index + len(answer_text)\n",
    "\n",
    "# Re-run the function on train and validation answers\n",
    "adjust_answer_end_indices(train_answers, train_contexts)\n",
    "adjust_answer_end_indices(val_answers, val_contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1dcd2e-311c-4e2a-9243-01112d969673",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasan2/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d722c6-ca1b-4055-b055-21121feb23f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_token_positions(encodings, answers):\n",
    "    start_positions, end_positions = [], []\n",
    "\n",
    "    for idx, answer in enumerate(answers):\n",
    "        start_positions.append(encodings.char_to_token(idx, answer['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(idx, answer['answer_end']))\n",
    "\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        shift = 1\n",
    "        while end_positions[-1] is None:\n",
    "            end_positions[-1] = encodings.char_to_token(idx, answer['answer_end'] - shift)\n",
    "            shift += 1\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "# Map token positions for both datasets\n",
    "map_token_positions(train_encodings, train_answers)\n",
    "map_token_positions(val_encodings, val_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf18b714-8691-4bac-b442-7e43538e6540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {key: torch.tensor(value[index]) for key, value in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d2bed2b-c986-43ba-9152-26a099c1505a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create train and validation datasets\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc76104-5401-44ec-98c6-a2281b4f5975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Setup device and optimizer\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "print(device)\n",
    "optim = AdamW(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e58f16a-b18b-4c3d-a06e-7cd3a43f0dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2320/2320 [09:07<00:00,  4.24it/s, loss=1.87]\n",
      "Epoch 1: 100%|██████████| 2320/2320 [09:03<00:00,  4.27it/s, loss=1.75] \n",
      "Epoch 2: 100%|██████████| 2320/2320 [09:03<00:00,  4.27it/s, loss=1.36] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/tokenizer_config.json',\n",
       " 'models/special_tokens_map.json',\n",
       " 'models/vocab.txt',\n",
       " 'models/added_tokens.json',\n",
       " 'models/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        start_positions=start_positions,\n",
    "                        end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Save model\n",
    "model_path = 'models/'\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58d7635a-3d11-4976-878e-ba8f0b017dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertForQuestionAnswering.from_pretrained(\"models/\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a90c55d-4a2b-424d-b1f6-10c7e0674d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 490/1116 [00:46<00:59, 10.52it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation on validation set\n",
    "model.eval()\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "acc, answers, references = [], [], []\n",
    "loop = tqdm(val_loader)\n",
    "\n",
    "for batch in loop:\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "        acc.append(((start_pred == start_true).sum() / len(start_pred)).item())\n",
    "        acc.append(((end_pred == end_true).sum() / len(end_pred)).item())\n",
    "\n",
    "        for i in range(start_pred.shape[0]):\n",
    "            all_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][i])\n",
    "            answer = ' '.join(all_tokens[start_pred[i]: end_pred[i] + 1])\n",
    "            ref = ' '.join(all_tokens[start_true[i]: end_true[i] + 1])\n",
    "            answer_ids = tokenizer.convert_tokens_to_ids(answer.split())\n",
    "            answer = tokenizer.decode(answer_ids)\n",
    "            answers.append(answer)\n",
    "            references.append(ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac7e79a8-cbdc-4626-8513-0cc283cad0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\\\b(a|an|the)\\\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b740d8ff-3e19-4c87-b66c-6fbf63ccf310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bfa2356-9a88-454e-a238-67b10afda849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    return max([metric_fn(prediction, gt) for gt in ground_truths])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2977078-a3bc-496f-b901-26107de6ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(prediction, ground_truth):\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gt_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_tokens)\n",
    "    recall = 1.0 * num_same / len(gt_tokens)\n",
    "    return (2 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c9237c2-cb24-4c01-9210-2ef8c4b446ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gold_answers, predictions):\n",
    "    f1 = exact_match = total = 0\n",
    "    for gt, prediction in zip(gold_answers, predictions):\n",
    "        total += 1\n",
    "        exact_match += metric_max_over_ground_truths(exact_match_score, prediction, [gt])\n",
    "        f1 += metric_max_over_ground_truths(f1_score, prediction, [gt])\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    return {'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d8d7731-f50e-4a13-93fb-c4c7bc4430f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'f1': 14.200461257580255}\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "evaluation_results = evaluate(references, answers)\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae0be7-d499-4678-bccb-a102a866ecfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
